{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6442086b",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f058785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from elice_utils import EliceUtils\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "\n",
    "# MNIST 데이터 세트를 불러옵니다.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \n",
    "\n",
    "# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.\n",
    "train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
    "test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
    "\n",
    "\n",
    "print(\"원본 학습용 이미지 데이터 형태: \",train_images.shape)\n",
    "print(\"원본 평가용 이미지 데이터 형태: \",test_images.shape)\n",
    "print(\"원본 학습용 label 데이터: \",train_labels)\n",
    "\n",
    "# 첫 번째 샘플 데이터를 출력합니다.\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(train_images[0], cmap=plt.cm.binary)\n",
    "plt.colorbar()\n",
    "plt.title(\"Training Data Sample\")\n",
    "plt.savefig(\"sample1.png\")\n",
    "elice_utils.send_image(\"sample1.png\")\n",
    "\n",
    "# 9개의 학습용 샘플 데이터를 출력합니다.\n",
    "class_names = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.savefig(\"sample2.png\")\n",
    "elice_utils.send_image(\"sample2.png\")\n",
    "\n",
    "\"\"\"\n",
    "1. CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.\n",
    "\"\"\"\n",
    "train_images = tf.expand_dims(train_images, -1)\n",
    "test_images = tf.expand_dims(test_images, -1)\n",
    "\n",
    "print(\"변환한 학습용 이미지 데이터 형태: \",train_images.shape)\n",
    "print(\"변환한 평가용 이미지 데이터 형태: \",test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9017abf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ece7d0e",
   "metadata": {},
   "source": [
    "## Keras에서 CNN 모델을 만들기 위해 필요한 함수/메서드\n",
    "\n",
    "1. CNN 레이어\n",
    "tf.keras.layers.Conv2D(filters, kernel_size, activation, padding)\n",
    "Copy\n",
    ": 입력 이미지의 특징, 즉 처리할 특징 맵(map)을 추출하는 레이어입니다.\n",
    "\n",
    "filters : 필터(커널) 개수\n",
    "kernel_size : 필터(커널)의 크기\n",
    "activation : 활성화 함수\n",
    "padding : 이미지가 필터를 거칠 때 그 크기가 줄어드는 것을 방지하기 위해서 가장자리에 0의 값을 가지는 픽셀을 넣을 것인지 말 것인지를 결정하는 변수. ‘SAME’ 또는 ‘VALID’\n",
    "\n",
    "2. Maxpool 레이어\n",
    "tf.keras.layers.MaxPool2D(padding)\n",
    "Copy\n",
    ": 처리할 특징 맵(map)의 크기를 줄여주는 레이어입니다.\n",
    "\n",
    "padding : ‘SAME’ 또는 ‘VALID’\n",
    "\n",
    "\n",
    "3. Flatten 레이어\n",
    "tf.keras.layers.Flatten()\n",
    "Copy\n",
    ": Convolution layer 또는 MaxPooling layer의 결과는 N차원의 텐서 형태입니다. 이를 1차원으로 평평하게 만들어줍니다.\n",
    "\n",
    "4. Dense 레이어\n",
    "tf.keras.layers.Dense(node, activation)\n",
    "Copy\n",
    "node : 노드(뉴런) 개수\n",
    "activation : 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d13228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from visual import *\n",
    "from elice_utils import EliceUtils\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "\n",
    "# MNIST 데이터 세트를 불러옵니다.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \n",
    "\n",
    "# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.\n",
    "train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
    "test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
    "\n",
    "# CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.\n",
    "train_images = tf.expand_dims(train_images, -1)\n",
    "test_images = tf.expand_dims(test_images, -1)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "1. CNN 모델을 설정합니다.\n",
    "   분류 모델에 맞게 마지막 레이어의 노드 수는 10개, activation 함수는 'softmax'로 설정합니다.\n",
    "\"\"\"\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)),\n",
    "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
    "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
    "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# CNN 모델 구조를 출력합니다.\n",
    "print(model.summary())\n",
    "\n",
    "# CNN 모델의 학습 방법을 설정합니다.\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "              \n",
    "# 학습을 수행합니다. \n",
    "history = model.fit(train_images, train_labels, epochs = 20, batch_size = 512)\n",
    "\n",
    "# 학습 결과를 출력합니다.\n",
    "Visulaize([('CNN', history)], 'loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dee8bd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da05c",
   "metadata": {},
   "source": [
    "## Keras에서 CNN 모델의 평가 및 예측을 위해 필요한 함수/메서드\n",
    "\n",
    "1. 평가 방법\n",
    "model.evaluate(X, Y)\n",
    "Copy\n",
    "evaluate() 메서드는 학습된 모델을 바탕으로 입력한 feature 데이터 X와 label Y의 loss 값과 metrics 값을 출력합니다.\n",
    "\n",
    "2. 예측 방법\n",
    "model.predict_classes(X)\n",
    "Copy\n",
    "X 데이터의 예측 label 값을 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37731d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def Plotter(test_images, model):\n",
    "\n",
    "    img_tensor = test_images[0]\n",
    "    img_tensor = np.expand_dims(img_tensor, axis=0) \n",
    "    \n",
    "    layer_outputs = [layer.output for layer in model.layers[:6]]\n",
    "    activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "\n",
    "    activations = activation_model.predict(img_tensor)\n",
    "    \n",
    "    layer_names = []\n",
    "    for layer in model.layers[:6]:\n",
    "        layer_names.append(layer.name)\n",
    "    \n",
    "    images_per_row = 16\n",
    "\n",
    "    for layer_name, layer_activation in zip(layer_names, activations):\n",
    "        n_features = layer_activation.shape[-1]\n",
    "    \n",
    "        size = layer_activation.shape[1]\n",
    "    \n",
    "        n_cols = n_features // images_per_row\n",
    "        display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "    \n",
    "        for col in range(n_cols):\n",
    "            for row in range(images_per_row):\n",
    "                channel_image = layer_activation[0, :, :, col * images_per_row + row]\n",
    "            \n",
    "                channel_image -= channel_image.mean() \n",
    "                channel_image /= channel_image.std()\n",
    "                channel_image *= 64\n",
    "                channel_image += 128\n",
    "                channel_image = np.clip(channel_image, 0, 255.).astype('uint8')\n",
    "            \n",
    "                display_grid[col * size : (col+1) * size, row * size : (row+1) * size] = channel_image\n",
    "            \n",
    "        scale = 1. / size\n",
    "        print('레이어 이름: ', layer_name)\n",
    "        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n",
    "        plt.savefig(\"plot.png\")\n",
    "        elice_utils.send_image(\"plot.png\")\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from visual import *\n",
    "from plotter import *\n",
    "from elice_utils import EliceUtils\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "\n",
    "# MNIST 데이터 세트를 불러옵니다.\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# MNIST 데이터 세트를 Train set과 Test set으로 나누어 줍니다.\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()    \n",
    "\n",
    "# Train 데이터 5000개와 Test 데이터 1000개를 사용합니다.\n",
    "train_images, train_labels = train_images[:5000], train_labels[:5000]\n",
    "test_images, test_labels = test_images[:1000], test_labels[:1000]\n",
    "\n",
    "# CNN 모델의 입력으로 사용할 수 있도록 (샘플개수, 가로픽셀, 세로픽셀, 1) 형태로 변환합니다.\n",
    "train_images = tf.expand_dims(train_images, -1)\n",
    "test_images = tf.expand_dims(test_images, -1)\n",
    "\n",
    "\n",
    "# CNN 모델을 설정합니다.\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME', input_shape = (28,28,1)),\n",
    "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
    "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', padding = 'SAME'),\n",
    "    tf.keras.layers.MaxPool2D(padding = 'SAME'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])\n",
    "\n",
    "# CNN 모델 구조를 출력합니다.\n",
    "print(model.summary())\n",
    "\n",
    "# CNN 모델의 학습 방법을 설정합니다.\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "              optimizer = 'adam',\n",
    "              metrics = ['accuracy'])\n",
    "              \n",
    "# 학습을 수행합니다. \n",
    "history = model.fit(train_images, train_labels, epochs = 10, batch_size = 128, verbose = 2)\n",
    "\n",
    "Visulaize([('CNN', history)], 'loss')\n",
    "\n",
    "\"\"\"\n",
    "1. 평가용 데이터를 활용하여 모델을 평가합니다.\n",
    "   loss와 accuracy를 계산하고 loss, test_acc에 저장합니다.\n",
    "\"\"\"\n",
    "loss, test_acc = model.evaluate(test_images, test_labels, verbose = 0)\n",
    "\n",
    "\"\"\"\n",
    "2. 평가용 데이터에 대한 예측 결과를 predictions에 저장합니다.\n",
    "\"\"\"\n",
    "predictions = model.predict_classes(test_images)\n",
    "\n",
    "# 모델 평가 및 예측 결과를 출력합니다.\n",
    "print('\\nTest Loss : {:.4f} | Test Accuracy : {}'.format(loss, test_acc))\n",
    "print('예측한 Test Data 클래스 : ',predictions[:10])\n",
    "\n",
    "# 평가용 데이터에 대한 레이어 결과를 시각화합니다.\n",
    "Plotter(test_images, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a175801",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "893b2319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보조\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "np_load_old = np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# 데이터를 불러오고 전처리하는 함수입니다.\n",
    "\n",
    "n_of_training_ex = 5000\n",
    "n_of_testing_ex = 1000\n",
    "\n",
    "PATH = \"./\"\n",
    "\n",
    "def imdb_data_load():\n",
    "\n",
    "    X_train = np.load(PATH + \"X_train.npy\")[:n_of_training_ex]\n",
    "    y_train = np.load(PATH + \"y_train.npy\")[:n_of_training_ex]\n",
    "    X_test = np.load(PATH + \"X_test.npy\")[:n_of_testing_ex]\n",
    "    y_test = np.load(PATH + \"y_test.npy\")[:n_of_testing_ex]\n",
    "\n",
    "    # 단어 사전 불러오기\n",
    "    with open(PATH+\"imdb_word_index.json\") as f:\n",
    "        word_index = json.load(f)\n",
    "    # 인덱스 -> 단어 방식으로 딕셔너리 설정\n",
    "    inverted_word_index = dict((i, word) for (word, i) in word_index.items())\n",
    "    # 인덱스를 바탕으로 문장으로 변환\n",
    "    decoded_sequence = \" \".join(inverted_word_index[i] for i in X_train[0])\n",
    "\n",
    "    \n",
    "    print(\"첫 번째 X_train 데이터 샘플 문장: \\n\",decoded_sequence)\n",
    "    print(\"\\n첫 번째 X_train 데이터 샘플 토큰 인덱스 sequence: \\n\",X_train[0])\n",
    "    print(\"첫 번째 X_train 데이터 샘플 토큰 시퀀스 길이: \", len(X_train[0]))\n",
    "    print(\"첫 번째 y_train 데이터: \",y_train[0])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862f2ede",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() got multiple values for keyword argument 'allow_pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-692d36d1de15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimdb_data_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \"\"\"\n",
      "\u001b[1;32m<ipython-input-7-f77dac3f52b5>\u001b[0m in \u001b[0;36mimdb_data_load\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimdb_data_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"X_train.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_of_training_ex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"y_train.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_of_training_ex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"X_test.npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_of_testing_ex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-f77dac3f52b5>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*a, **k)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mnp_load_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp_load_old\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# 데이터를 불러오고 전처리하는 함수입니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-f77dac3f52b5>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*a, **k)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mnp_load_old\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp_load_old\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_pickle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# 데이터를 불러오고 전처리하는 함수입니다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() got multiple values for keyword argument 'allow_pickle'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import data_process\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.\n",
    "X_train, y_train, X_test, y_test = data_process.imdb_data_load()\n",
    "\n",
    "\"\"\"\n",
    "1. 인덱스로 변환된 X_train, X_test 시퀀스에 패딩을 수행하고 각각 X_train, X_test에 저장합니다.\n",
    "   시퀀스 최대 길이는 300으로 설정합니다.\n",
    "\"\"\"\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=300, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=300, padding='post')\n",
    "\n",
    "print(\"\\n패딩을 추가한 첫 번째 X_train 데이터 샘플 토큰 인덱스 sequence: \\n\",X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e2aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_process\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.\n",
    "X_train, y_train, X_test, y_test = data_process.imdb_data_load()\n",
    "\n",
    "max_review_length = 300\n",
    "\n",
    "# 패딩을 수행합니다.\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')\n",
    "\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "\"\"\"\n",
    "1. 모델을 구현합니다.\n",
    "   임베딩 레이어 다음으로 `SimpleRNN`을 사용하여 RNN 레이어를 쌓고 노드의 개수는 5개로 설정합니다. \n",
    "   Dense 레이어는 0, 1 분류이기에 노드를 1개로 하고 activation을 'sigmoid'로 설정되어 있습니다.\n",
    "\"\"\"\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length),\n",
    "    tf.keras.layers.SimpleRNN(5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "# 모델을 확인합니다.\n",
    "print(model.summary())\n",
    "\n",
    "# 학습 방법을 설정합니다.\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# 학습을 수행합니다.\n",
    "model_history = model.fit(X_train, y_train, epochs = 3, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3986c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import data_process\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# 동일한 실행 결과 확인을 위한 코드입니다.\n",
    "np.random.seed(123)\n",
    "tf.random.set_seed(123)\n",
    "\n",
    "# 학습용 및 평가용 데이터를 불러오고 샘플 문장을 출력합니다.\n",
    "X_train, y_train, X_test, y_test = data_process.imdb_data_load()\n",
    "\n",
    "max_review_length = 300\n",
    "\n",
    "# 패딩을 수행합니다.\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length, padding='post')\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length, padding='post')\n",
    "\n",
    "\n",
    "embedding_vector_length = 32\n",
    "\n",
    "\n",
    "# 모델을 구현합니다.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(1000, embedding_vector_length, input_length = max_review_length),\n",
    "    tf.keras.layers.SimpleRNN(5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "# 모델을 확인합니다.\n",
    "print(model.summary())\n",
    "\n",
    "# 학습 방법을 설정합니다.\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "# 학습을 수행합니다.\n",
    "model_history = model.fit(X_train, y_train, epochs = 5, verbose = 2)\n",
    "\n",
    "\"\"\"\n",
    "1. 평가용 데이터를 활용하여 모델을 평가합니다.\n",
    "   loss와 accuracy를 계산하고 loss, test_acc에 저장합니다.\n",
    "\"\"\"\n",
    "loss, test_acc = model.evaluate(X_test, y_test, verbose = 0)\n",
    "\n",
    "\"\"\"\n",
    "2. 평가용 데이터에 대한 예측 결과를 predictions에 저장합니다.\n",
    "\"\"\"\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# 모델 평가 및 예측 결과를 출력합니다.\n",
    "print('\\nTest Loss : {:.4f} | Test Accuracy : {}'.format(loss, test_acc))\n",
    "print('예측한 Test Data 클래스 : ',1 if predictions[0]>=0.5 else 0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
