{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e0fcb54",
   "metadata": {},
   "source": [
    "## 산업별 AI 혁신 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea51b13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fc6e8",
   "metadata": {},
   "source": [
    "### 데이터 확인하고 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8158b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 데이터를 읽고 처리하는 코드를 작성해보세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train_us, x_test_us, y_train_us, y_test_us = ma.preprocess()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 2번. 학습을 수행시켜보세요.\n",
    "    \"\"\"\n",
    "    model = ma.train(x_train_us, y_train_us)\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee79877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import sys\n",
    "from tqdm import trange\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, average_precision_score\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    print(\"데이터 읽는 중...\")\n",
    "    data = pd.read_csv('data/uci-secom.csv')\n",
    "    \n",
    "    print(\"읽어 온 데이터를 출력합니다.\")\n",
    "    print(data)\n",
    "    \n",
    "    #print(data.isnull().sum())\n",
    "    #print(\"결측 데이터 값을 처리합니다.\")\n",
    "    data = data.replace(np.NaN, 0)\n",
    "    #print(data.isnull().sum())\n",
    "    \n",
    "    # 쓸모없는 데이터 지우기\n",
    "    data = data.drop(columns = ['Time'], axis = 1)\n",
    "    \n",
    "    # 타겟 데이터 분리\n",
    "    x = data.iloc[:,:590]\n",
    "    y = data.iloc[:, 590]\n",
    "\n",
    "    print(\"\\n학습용 데이터와 테스트용 데이터로 분리 합니다.\")\n",
    "\n",
    "    # Under sampling 수행\n",
    "    failed_tests = np.array(data[data['Pass/Fail'] == 1].index)\n",
    "    no_failed_tests = len(failed_tests)\n",
    "\n",
    "    normal_indices = data[data['Pass/Fail'] == -1]\n",
    "    no_normal_indices = len(normal_indices)\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    random_normal_indices = np.random.choice(no_normal_indices, size = no_failed_tests, replace = True)\n",
    "    random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "    under_sample = np.concatenate([failed_tests, random_normal_indices])\n",
    "    undersample_data = data.iloc[under_sample, :]\n",
    "\n",
    "    x = undersample_data.iloc[:, undersample_data.columns != 'Pass/Fail'] \n",
    "    y = undersample_data.iloc[:, undersample_data.columns == 'Pass/Fail']\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    x_train_us, x_test_us, y_train_us, y_test_us = train_test_split(x, y, test_size = 0.2, random_state = 4)\n",
    "    \n",
    "    print(\"학습용 데이터 크기: {}\".format(x_train_us.shape))\n",
    "    print(\"테스트용 데이터 크기: {}\".format(x_test_us.shape))\n",
    "    \n",
    "    return x_train_us, x_test_us, y_train_us, y_test_us\n",
    "\n",
    "def train(x_train_us, y_train_us):\n",
    "\n",
    "    print(\"학습을 수행합니다.\")\n",
    "    \n",
    "    model = XGBClassifier(random_state=2)\n",
    "    \n",
    "    # 파라미터 튜닝\n",
    "    parameters = [{'max_depth' : [1, 2, 3, 4, 5, 6]}]\n",
    "    \n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'recall', cv = 4, n_jobs = -1)\n",
    "    \n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    for j in trange(5000,file=sys.stdout, leave=False, unit_scale=True, desc='학습 진행률'):\n",
    "    \n",
    "        with open('model.pkl', 'rb') as f:\n",
    "             model = pickle.load(f)\n",
    "    print(\"학습 완료\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4f9dd",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cf3b2b",
   "metadata": {},
   "source": [
    "### 학습 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6924d401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 예측 정확도 결과를 출력해보세요.\n",
    "    \"\"\"\n",
    "    ma.evaluation()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedd924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, average_precision_score\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    #print(\"데이터 읽는 중...\")\n",
    "    data = pd.read_csv('data/uci-secom.csv')\n",
    "    \n",
    "    #print(data.isnull().sum())\n",
    "    #print(\"결측 데이터 값을 처리합니다.\")\n",
    "    data = data.replace(np.NaN, 0)\n",
    "    #print(data.isnull().sum())\n",
    "    \n",
    "    # 쓸모없는 데이터 지우기\n",
    "    data = data.drop(columns = ['Time'], axis = 1)\n",
    "    \n",
    "    # 타겟 데이터 분리\n",
    "    x = data.iloc[:,:590]\n",
    "    y = data.iloc[:, 590]\n",
    "\n",
    "    #print(\"학습용 데이터와 테스트용 데이터로 분리 합니다.\")\n",
    "\n",
    "    # Under sampling 수행\n",
    "    failed_tests = np.array(data[data['Pass/Fail'] == 1].index)\n",
    "    no_failed_tests = len(failed_tests)\n",
    "\n",
    "    normal_indices = data[data['Pass/Fail'] == -1]\n",
    "    no_normal_indices = len(normal_indices)\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    random_normal_indices = np.random.choice(no_normal_indices, size = no_failed_tests, replace = True)\n",
    "    random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "    under_sample = np.concatenate([failed_tests, random_normal_indices])\n",
    "    undersample_data = data.iloc[under_sample, :]\n",
    "\n",
    "    x = undersample_data.iloc[:, undersample_data.columns != 'Pass/Fail'] \n",
    "    y = undersample_data.iloc[:, undersample_data.columns == 'Pass/Fail']\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    x_train_us, x_test_us, y_train_us, y_test_us = train_test_split(x, y, test_size = 0.2, random_state = 4)\n",
    "    \n",
    "    #print(\"학습용 데이터 크기: {}\".format(x_train_us.shape))\n",
    "    #print(\"테스트용 데이터 크기: {}\".format(x_test_us.shape))\n",
    "    \n",
    "    return x_train_us, x_test_us, y_train_us, y_test_us\n",
    "    \n",
    "    \n",
    "def train(x_train_us, y_train_us):\n",
    "\n",
    "    \"\"\"\n",
    "    #print(\"학습을 수행합니다.\")\n",
    "    \n",
    "    model = XGBClassifier(random_state=2)\n",
    "    \n",
    "    # 파라미터 튜닝\n",
    "    parameters = [{'max_depth' : [1, 2, 3, 4, 5, 6]}]\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'recall', cv = 4, n_jobs = -1)\n",
    "\n",
    "    grid_search = grid_search.fit(x_train_us, y_train_us)\n",
    "    \n",
    "    # 베스트 모델 선택\n",
    "    model = grid_search.best_estimator_\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    with open('model.pkl', 'rb') as f:\n",
    "         model = pickle.load(f)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def evaluation():\n",
    "\n",
    "    x_train_us, x_test_us, y_train_us, y_test_us = preprocess()\n",
    "    model = train(x_train_us, y_train_us)\n",
    "    # 테스트 데이터 예측\n",
    "    y_pred = model.predict(x_test_us)\n",
    "    print('평가 지표인 recall score를 출력합니다.: ', recall_score(y_test_us, y_pred), '\\n')\n",
    "    \n",
    "    print(\"센서들의 중요도를 출력합니다.\")\n",
    "    xgb.plot_importance(model, height = 1, grid = True, importance_type = 'gain', show_values = False, max_num_features = 20)\n",
    "\n",
    "    plt.rcParams['figure.figsize'] = (10, 15)\n",
    "    plt.xlabel('The importance score for each features')\n",
    "    plt.ylabel('Features')\n",
    "    \n",
    "    plt.savefig(\"result1.png\")\n",
    "    elice_utils.send_image(\"result1.png\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da113c7",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97670008",
   "metadata": {},
   "source": [
    "### 공정 이상 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c8cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "\t\n",
    "    \"\"\"\n",
    "    지시사항 1번. 103번 센서값인 아래의 value_103_sensor 값을 바꾸어보세요.\n",
    "    \"\"\"\n",
    "    value_103_sensor = -40\n",
    "    \n",
    "    # 예측을 진행하는 코드입니다.\n",
    "    ma.predict(value_103_sensor)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7bf622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, average_precision_score\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    #print(\"데이터 읽는 중...\")\n",
    "    data = pd.read_csv('data/uci-secom.csv')\n",
    "    \n",
    "    #print(data.isnull().sum())\n",
    "    #print(\"결측 데이터 값을 처리합니다.\")\n",
    "    data = data.replace(np.NaN, 0)\n",
    "    #print(data.isnull().sum())\n",
    "    \n",
    "    # 쓸모없는 데이터 지우기\n",
    "    data = data.drop(columns = ['Time'], axis = 1)\n",
    "    \n",
    "    # 타겟 데이터 분리\n",
    "    x = data.iloc[:,:590]\n",
    "    y = data.iloc[:, 590]\n",
    "\n",
    "    #print(\"학습용 데이터와 테스트용 데이터로 분리 합니다.\")\n",
    "\n",
    "    # Under sampling 수행\n",
    "    failed_tests = np.array(data[data['Pass/Fail'] == 1].index)\n",
    "    no_failed_tests = len(failed_tests)\n",
    "\n",
    "    normal_indices = data[data['Pass/Fail'] == -1]\n",
    "    no_normal_indices = len(normal_indices)\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    random_normal_indices = np.random.choice(no_normal_indices, size = no_failed_tests, replace = True)\n",
    "    random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "    under_sample = np.concatenate([failed_tests, random_normal_indices])\n",
    "    undersample_data = data.iloc[under_sample, :]\n",
    "\n",
    "    x = undersample_data.iloc[:, undersample_data.columns != 'Pass/Fail'] \n",
    "    y = undersample_data.iloc[:, undersample_data.columns == 'Pass/Fail']\n",
    "    y = np.ravel(y)\n",
    "\n",
    "    x_train_us, x_test_us, y_train_us, y_test_us = train_test_split(x, y, test_size = 0.2, random_state = 4)\n",
    "    \n",
    "    #print(\"학습용 데이터 크기: {}\".format(x_train_us.shape))\n",
    "    #print(\"테스트용 데이터 크기: {}\".format(x_test_us.shape))\n",
    "    \n",
    "    return x_train_us, x_test_us, y_train_us, y_test_us, data\n",
    "    \n",
    "    \n",
    "def train(x_train_us, y_train_us):\n",
    "\n",
    "    \"\"\"\n",
    "    #print(\"학습을 수행합니다.\")\n",
    "    \n",
    "    model = XGBClassifier(random_state=2)\n",
    "    \n",
    "    # 파라미터 튜닝\n",
    "    parameters = [{'max_depth' : [1, 2, 3, 4, 5, 6]}]\n",
    "\n",
    "    grid_search = GridSearchCV(estimator = model, param_grid = parameters, scoring = 'recall', cv = 4, n_jobs = -1)\n",
    "\n",
    "    grid_search = grid_search.fit(x_train_us, y_train_us)\n",
    "    \n",
    "    # 베스트 모델 선택\n",
    "    model = grid_search.best_estimator_\n",
    "    \"\"\"\n",
    "    import pickle\n",
    "    \n",
    "    with open('model.pkl', 'rb') as f:\n",
    "         model = pickle.load(f)\n",
    "    \n",
    "    return model\n",
    "    \n",
    "def predict(value_103_sensor):\n",
    "\n",
    "    x_train_us, x_test_us, y_train_us, y_test_us, data = preprocess()\n",
    "    model = train(x_train_us, y_train_us)\n",
    "    \n",
    "    pre_data = data\n",
    "    pre_data.loc[1242, '103'] = value_103_sensor\n",
    "    pre_data = pre_data[1242:1243]\n",
    "    \n",
    "    prediction = model.predict(pre_data.drop(columns = 'Pass/Fail'))\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    sns.distplot(data['103'], color = 'darkblue')\n",
    "    plt.title('103 Sensor Measurements', fontsize = 20)\n",
    "    \n",
    "    ax.annotate('103_sensor_value',\n",
    "            xy=(value_103_sensor, 0), xycoords='data',\n",
    "            xytext=(10, 30), textcoords='offset points',\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            horizontalalignment='left', verticalalignment='bottom')\n",
    "    \n",
    "    plt.savefig(\"result1.png\")\n",
    "    elice_utils.send_image(\"result1.png\")\n",
    "    \n",
    "    if prediction == 1:\n",
    "    \n",
    "        print(\"103 센서 데이터 값이 {}인 경우 공정 이상이 발생할 것으로 예측됩니다.\".format(value_103_sensor))\n",
    "    else:\n",
    "        print(\"103 센서 데이터 값이 {}인 경우 공정 이상이 발생하지 않을 것으로 예측됩니다.\".format(value_103_sensor))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab2745a",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 이커머스 산업의 AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8287ae",
   "metadata": {},
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03782fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 인공지능 모델 학습을 수행해보세요.\n",
    "    \"\"\"\n",
    "    netflix_overall, cosine_sim, indices = ma.preprocess()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492339a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "from tqdm import trange\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clean_data(x):\n",
    "    return str.lower(x.replace(\" \", \"\"))\n",
    "\n",
    "def create_soup(x):\n",
    "    return x['title']+ ' ' + x['director'] + ' ' + x['cast'] + ' ' +x['listed_in']+' '+ x['description']\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    print(\"데이터 읽는 중...\")\n",
    "    netflix_overall=pd.read_csv(\"data/netflix_titles.csv\")\n",
    "\n",
    "    print(\"읽어 온 데이터를 출력합니다.\")\n",
    "    print(netflix_overall)\n",
    "    \n",
    "    filledna=netflix_overall.fillna('')\n",
    "\n",
    "    features=['title','director','cast','listed_in','description']\n",
    "    filledna=filledna[features]\n",
    "\n",
    "    for feature in features:\n",
    "        filledna[feature] = filledna[feature].apply(clean_data)\n",
    "\n",
    "    filledna['soup'] = filledna.apply(create_soup, axis=1)\n",
    "    \n",
    "    count = CountVectorizer(stop_words='english')\n",
    "    count_matrix = count.fit_transform(filledna['soup'])\n",
    "\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "    filledna=filledna.reset_index()\n",
    "    indices = pd.Series(filledna.index, index=filledna['title'])\n",
    "    \n",
    "    print(\"학습을 수행합니다.\")\n",
    "    \n",
    "    for j in trange(20,file=sys.stdout, leave=False, unit_scale=True, desc='학습 진행률'):\n",
    "        \n",
    "        cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "    \n",
    "    print('학습이 완료되었습니다.')\n",
    "    return netflix_overall, cosine_sim, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ce2a3f",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6084834e",
   "metadata": {},
   "source": [
    "### 추천 알고리즘 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a4584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    netflix_overall, cosine_sim, indices = ma.preprocess()\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 따옴표 사이에 들어가 있는 영화명을 지우고 왼쪽 지문의 예시 중 원하는 영화명을 입력해보세요.\n",
    "    \"\"\"\n",
    "    title = 'Pororo - The Little Penguin'\n",
    "    \n",
    "    print(\"{}와 비슷한 넷플릭스 콘텐츠를 추천합니다.\".format(title))\n",
    "    ma.get_recommendations_new(title, netflix_overall, cosine_sim, indices)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clean_data(x):\n",
    "    return str.lower(x.replace(\" \", \"\"))\n",
    "\n",
    "def create_soup(x):\n",
    "    return x['title']+ ' ' + x['director'] + ' ' + x['cast'] + ' ' +x['listed_in']+' '+ x['description']\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    netflix_overall=pd.read_csv(\"data/netflix_titles.csv\")\n",
    "\n",
    "    filledna=netflix_overall.fillna('')\n",
    "\n",
    "    features=['title','director','cast','listed_in','description']\n",
    "    filledna=filledna[features]\n",
    "\n",
    "    for feature in features:\n",
    "        filledna[feature] = filledna[feature].apply(clean_data)\n",
    "\n",
    "    filledna['soup'] = filledna.apply(create_soup, axis=1)\n",
    "    \n",
    "    count = CountVectorizer(stop_words='english')\n",
    "    count_matrix = count.fit_transform(filledna['soup'])\n",
    "\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "    filledna=filledna.reset_index()\n",
    "    indices = pd.Series(filledna.index, index=filledna['title'])\n",
    "    \n",
    "    return netflix_overall, cosine_sim, indices\n",
    "    \n",
    "    \n",
    "def get_recommendations_new(title, netflix_overall, cosine_sim, indices):\n",
    "    \n",
    "    pd.set_option('display.max_columns', None)\n",
    "    title=title.replace(' ','').lower()\n",
    "    \n",
    "    try:\n",
    "        idx = indices[title]\n",
    "\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        sim_scores = sim_scores[1:11]\n",
    "\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "        recomendation = netflix_overall[['title','country','release_year']].iloc[movie_indices]\n",
    "        \n",
    "        sim_pd = pd.DataFrame(sim_scores)[[1]]\n",
    "        \n",
    "        sim_pd = sim_pd.rename(columns={1:'Similiarity'})\n",
    "\n",
    "        recomendation = recomendation.reset_index(drop=True)\n",
    "\n",
    "        recomendation = pd.concat([recomendation, sim_pd], axis=1)\n",
    "    \n",
    "        recomendation.index += 1\n",
    "        \n",
    "        \n",
    "        return print(recomendation)\n",
    "    \n",
    "    except:\n",
    "        print(\"오류: 올바른 title 명을 적어주세요.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71817435",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## 웹/앱 서비스 산업 AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace46fcb",
   "metadata": {},
   "source": [
    "### 이미지 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6524978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 손글씨 이미지에서 숫자 부분만 분리하는 과정을 수행해보세요.\n",
    "    \"\"\"\n",
    "    ma.data_print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01818a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "    \n",
    "    \n",
    "def data_print():\n",
    "    \n",
    "    img = cv2.imread(\"data/numbers.jpg\")\n",
    "    \n",
    "    print(\"원본 이미지를 출력합니다.\")\n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.imshow(img);\n",
    "    \n",
    "    plt.savefig(\"result2.png\")\n",
    "    elice_utils.send_image(\"result2.png\")\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 0)\n",
    "    img_th = cv2.threshold(img_blur, 155, 250, cv2.THRESH_BINARY_INV)[1]\n",
    "    \n",
    "    contours, hierachy= cv2.findContours(img_th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    rects = [cv2.boundingRect(each) for each in contours]\n",
    "    \n",
    "    tmp = [w*h for (x,y,w,h) in rects]\n",
    "    tmp.sort()\n",
    "    \n",
    "    rects = [(x,y,w,h) for (x,y,w,h) in rects if ((w*h>1000)and(w*h<500000))]\n",
    "    \n",
    "    print(\"\\n이미지를 분할할 영역을 표시합니다.\")\n",
    "    for rect in rects:\n",
    "    # Draw the rectangles\n",
    "        cv2.rectangle(img, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (0, 255, 0), 5) \n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(15,12))\n",
    "    plt.imshow(img);\n",
    "    plt.savefig(\"result3.png\")\n",
    "    elice_utils.send_image(\"result3.png\")\n",
    "    \n",
    "    seg_img = []\n",
    "\n",
    "    margin_pixel = 50\n",
    "\n",
    "    for cnt in contours: \n",
    "        x, y, w, h = cv2.boundingRect(cnt) \n",
    "\n",
    "        # Drawing a rectangle on copied image \n",
    "        if ((w*h>1000)and(w*h<500000)): \n",
    "\n",
    "            # Cropping the text block for giving input to OCR \n",
    "            cropped = img.copy()[y - margin_pixel:y + h + margin_pixel, x - margin_pixel:x + w + margin_pixel] \n",
    "            seg_img.append(cropped)\n",
    "\n",
    "            rect = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 5)\n",
    "\n",
    "    print(\"\\n분할 된 이미지를 출력합니다.\")\n",
    "    for i in range(len(seg_img)):\n",
    "        plt.clf()\n",
    "        plt.imshow(seg_img[i]);\n",
    "        plt.savefig(\"result4.png\")\n",
    "        elice_utils.send_image(\"result4.png\")\n",
    "    \n",
    "    \n",
    "    re_seg_img = []\n",
    "\n",
    "    for i in range(len(seg_img)):\n",
    "        re_seg_img.append(cv2.resize(seg_img[i], (28,28), interpolation=cv2.INTER_AREA))\n",
    "    \n",
    "    gray = cv2.cvtColor(re_seg_img[0], cv2.COLOR_BGR2GRAY)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57587d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 손글씨 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. MNIST 데이터를 학습하는 인공지능 모델을 구현해보세요.\n",
    "    \"\"\"\n",
    "    model = ma.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def train():\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # model에 batch normalization layer 추가\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "      #tf.keras.layers.Dense(10, activation='relu')\n",
    "    ])\n",
    "\n",
    "    # adam외의 optimizer로 변경\n",
    "    # sparse_categorical_crossentropy외의 loss로 변경\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # epochs 값 변경\n",
    "    model.fit(x_train, y_train, epochs=5)\n",
    "\n",
    "    model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "    # 임의의 5가지 test data의 이미지와 레이블값을 출력하고 예측된 레이블값 출력\n",
    "    predictions = model.predict(x_test)\n",
    "    idx_n = [15, 34, 68, 75, 98]\n",
    "\n",
    "    for i in idx_n:\n",
    "\n",
    "        img = x_test[i].reshape(28,28)\n",
    "        plt.imshow(img,cmap=\"gray\")\n",
    "        plt.show()\n",
    "        plt.savefig(\"result1.png\")\n",
    "        elice_utils.send_image(\"result1.png\")\n",
    "\n",
    "        print(\"Label: \", y_test[i])\n",
    "        print(\"Prediction: \", np.argmax(predictions[i]))\n",
    "        \n",
    "    return model\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aff8ee4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2422e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 이미지 변환 과정과 예측 과정을 수행해보세요.\n",
    "    \"\"\"\n",
    "    ma.data_predit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cdad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "    \n",
    "def data_predit():\n",
    "\n",
    "    # model에 batch normalization layer 추가\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "      #tf.keras.layers.Dense(10, activation='relu')\n",
    "    ])\n",
    "\n",
    "    # adam외의 optimizer로 변경\n",
    "    # sparse_categorical_crossentropy외의 loss로 변경\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    import os\n",
    "\n",
    "    checkpoint_path = \"./cp.ckpt\"\n",
    "    checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "    \n",
    "    model.load_weights(checkpoint_path)\n",
    "\n",
    "\n",
    "    \n",
    "    img = cv2.imread(\"data/numbers.jpg\")\n",
    "    \n",
    "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_blur = cv2.GaussianBlur(img_gray, (5, 5), 0)\n",
    "    img_th = cv2.threshold(img_blur, 150, 250, cv2.THRESH_BINARY_INV)[1]\n",
    "    \n",
    "    contours, hierachy= cv2.findContours(img_th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "\n",
    "    rects = [cv2.boundingRect(each) for each in contours]\n",
    "    \n",
    "    tmp = [w*h for (x,y,w,h) in rects]\n",
    "    tmp.sort()\n",
    "    \n",
    "    rects = [(x,y,w,h) for (x,y,w,h) in rects if ((w*h>1000)and(w*h<500000))]\n",
    "    \n",
    "    for rect in rects:\n",
    "    # Draw the rectangles\n",
    "        cv2.rectangle(img, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (0, 255, 0), 5) \n",
    "    \n",
    "    \n",
    "    seg_img = []\n",
    "\n",
    "    margin_pixel = 50\n",
    "\n",
    "    for cnt in contours: \n",
    "        x, y, w, h = cv2.boundingRect(cnt) \n",
    "\n",
    "        # Drawing a rectangle on copied image \n",
    "        if ((w*h>1500)and(w*h<600000)): \n",
    "\n",
    "            # Cropping the text block for giving input to OCR \n",
    "            cropped = img.copy()[y - margin_pixel:y + h + margin_pixel, x - margin_pixel:x + w + margin_pixel] \n",
    "            seg_img.append(cropped)\n",
    "\n",
    "            rect = cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 255), 5)\n",
    "\n",
    "    re_seg_img = []\n",
    "\n",
    "    for i in range(len(seg_img)):\n",
    "        re_seg_img.append(cv2.resize(seg_img[i], (28,28), interpolation=cv2.INTER_AREA))\n",
    "    \n",
    "    gray = cv2.cvtColor(re_seg_img[0], cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    for i in range(len(seg_img)):\n",
    "    \n",
    "        gray = cv2.cvtColor(re_seg_img[i], cv2.COLOR_BGR2GRAY)\n",
    "        img_binary = cv2.threshold(gray, 150, 250, cv2.THRESH_BINARY_INV)[1]\n",
    "        test = img_binary.reshape(1,28,28) / 255.0\n",
    "\n",
    "        #print(len(test))\n",
    "\n",
    "        predictions = model.predict(test)\n",
    "\n",
    "        img_test = test.reshape(28,28)\n",
    "        plt.clf()\n",
    "        \n",
    "        plt.subplot(121)\n",
    "        plt.imshow(seg_img[i])\n",
    "        plt.title('Origin')\n",
    "\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(img_test,cmap=\"gray\")\n",
    "        plt.title('Coverted')\n",
    "        plt.show()\n",
    "        \n",
    "        plt.savefig(\"result4.png\")\n",
    "        elice_utils.send_image(\"result4.png\")\n",
    "\n",
    "        #print(\"Label: \", y_test[i])\n",
    "        print(\"Prediction: \", np.argmax(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef73599",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "## 금융 산업 AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97981d7d",
   "metadata": {},
   "source": [
    "### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 출력된 데이터를 확인하고 학습을 수행해보세요.\n",
    "    \"\"\"\n",
    "    ma.data_plot()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb25d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import base64 as _base64\n",
    "import fcntl as _fcntl\n",
    "import mimetypes as _mimetypes\n",
    "import os.path as _ospath\n",
    "import struct as _struct\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition\n",
    "import sklearn.preprocessing\n",
    "import sklearn.cluster\n",
    "\n",
    "from elice_utils import EliceUtils  # isort:skip\n",
    "\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def display_digits(X, title):\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(X, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.savefig(\"image.svg\", format=\"svg\")\n",
    "    elice_utils.send_image(\"image.svg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75b45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Activation\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def data_plot():\n",
    "\n",
    "    # --- 주식 데이터 로드, 전처리, 분할, 모델 학습하기(이전 실습에서 진행) --- #\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "\n",
    "    # 데이터프레임 출력(데이터프레임은 (헹 X 열)로 이루어진 표 형태의 특수한 데이터 타입)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "    # --- 주식 데이터 살펴보기 --- #\n",
    "\n",
    "    print('\\n주식 데이터의 형태를 출력')\n",
    "    print(df.shape)\n",
    "\n",
    "    print('\\n주식 데이터의 정보를 출력')\n",
    "    print(df.info)\n",
    "\n",
    "    print('\\n주식 데이터의 상단 5개 행을 출력')\n",
    "    print(df.head())\n",
    "\n",
    "    print('\\n주식 데이터의 하단 5개 행을 출력')\n",
    "    print(df.tail())\n",
    "\n",
    "    print('\\n주식 데이터의 모든 열을 출력')\n",
    "    print(df.columns)\n",
    "\n",
    "    print('\\n주식 데이터의 요약 통계 자료 출력')\n",
    "    print(df.describe())\n",
    "\n",
    "def train():\n",
    "\n",
    "    # --- 주식 데이터 로드, 전처리, 분할, 모델 학습하기(이전 실습에서 진행) --- #\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "\n",
    "    # 주가의 중간값 계산하기\n",
    "    high_prices = df['High'].values\n",
    "    low_prices = df['Low'].values\n",
    "    mid_prices = (high_prices + low_prices) / 2\n",
    "\n",
    "    # 주가 데이터에 중간 값 요소 추가하기\n",
    "    df['Mid'] = mid_prices\n",
    "\n",
    "    # 종가의 5일 이동평균값을 계산하고 주가 데이터에 추가하기\n",
    "    ma5 = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['MA5'] = ma5\n",
    "\n",
    "    df = df.fillna(0) # 비어있는 값을 모두 0으로 바꾸기\n",
    "\n",
    "    # Date 열를 제거합니다.\n",
    "    df = df.drop('Date', axis = 1)\n",
    "\n",
    "    # 데이터 스케일링(MinMaxScaler 적용)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    fitted = min_max_scaler.fit(df)\n",
    "\n",
    "    output = min_max_scaler.transform(df)\n",
    "    output = pd.DataFrame(output, columns=df.columns, index=list(df.index.values))\n",
    "\n",
    "    # 트레인셋/테스트셋 크기 설정\n",
    "    train_size = int(len(output)* 0.6) # 트레인셋은 전체의 60%\n",
    "    test_size = int(len(output)*0.3) + train_size # 테스트셋은 전체의 30%\n",
    "\n",
    "    #train/test 학습 및 라벨 설정\n",
    "    #종가를 예측하기 위해 종가를 label로 설정\n",
    "    train_x = np.array(output[:train_size])\n",
    "    train_y = np.array(output['Close'][:train_size])\n",
    "    test_x =np.array(output[train_size:test_size])\n",
    "    test_y = np.array(output['Close'][train_size:test_size])\n",
    "    validation_x = np.array(output[test_size:])\n",
    "    validation_y = np.array(output['Close'][test_size:])\n",
    "\n",
    "    # Keras 모델을 생성합니다.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Keras 딥러닝 모델 학습을 위한 파라미터(옵션값)을 설정합니다.\n",
    "    learning_rate = 0.01\n",
    "    training_cnt = 1000\n",
    "    batch_size = 100 \n",
    "    input_size = 8 \n",
    "\n",
    "    # 생성된 딥러닝 모델에 학습용 데이터(train_x)를 넣습니다.\n",
    "    model.add(Dense(input_size, activation='tanh', input_shape=(train_x.shape[1],))) \n",
    "    model.add(Dense(input_size * 3,  activation='tanh')) \n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "    # 데이터를 학습을 진행합니다.\n",
    "    model.compile(optimizer='sgd', loss='mse', metrics=['mae', 'mape','acc'])\n",
    "    model.summary()\n",
    "    history = model.fit(train_x, train_y, epochs=training_cnt,   \n",
    "                        batch_size=batch_size, verbose=1)\n",
    "    val_mse, val_mae, val_mape, val_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "    \n",
    "    \n",
    "def predict():\n",
    "\n",
    "    # --- 학습 결과를 그래프로 확인해봅니다 --- #\n",
    "\n",
    "    # 학습된 모델로부터 테스트 데이터를 예측합니다.\n",
    "    pred = model.predict(test_x)\n",
    "\n",
    "    fig = plt.figure(facecolor='white', figsize=(8, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(test_y, label='True') # 실제 주가\n",
    "    ax.plot(pred, label='Prediction') # 우리가 만든 딥러닝 모델이 예측한 주가\n",
    "    ax.legend()\n",
    "\n",
    "    # 현재까지 그려진 그래프를 시각화\n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3ee872",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### 파생 변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b8950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 이동평균값(MA),거래량 이동평균값(VMA), 이격도값(disp) 변수를 추가해보세요.\n",
    "    \"\"\"\n",
    "    ma.data_preprocess()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f82017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Dense, Activation\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def data_plot():\n",
    "\n",
    "    # --- 주식 데이터 로드, 전처리, 분할, 모델 학습하기(이전 실습에서 진행) --- #\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "\n",
    "    # 데이터프레임 출력(데이터프레임은 (헹 X 열)로 이루어진 표 형태의 특수한 데이터 타입)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "    # --- 주식 데이터 살펴보기 --- #\n",
    "\n",
    "    print('\\n주식 데이터의 형태를 출력')\n",
    "    print(df.shape)\n",
    "\n",
    "    print('\\n주식 데이터의 정보를 출력')\n",
    "    print(df.info)\n",
    "\n",
    "    print('\\n주식 데이터의 상단 5개 행을 출력')\n",
    "    print(df.head())\n",
    "\n",
    "    print('\\n주식 데이터의 하단 5개 행을 출력')\n",
    "    print(df.tail())\n",
    "\n",
    "    print('\\n주식 데이터의 모든 열을 출력')\n",
    "    print(df.columns)\n",
    "\n",
    "    print('\\n주식 데이터의 요약 통계 자료 출력')\n",
    "    print(df.describe())\n",
    "    \n",
    "def data_preprocess():\n",
    "\n",
    "    # 주식 데이터 불러오기\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "\n",
    "\n",
    "    # 수정 종가 이동평균(MA: Moving Average) 값 구하기\n",
    "    ma5 = df['Adj Close'].rolling(window=5).mean() # 수정 종가 5일 이동평균\n",
    "    ma20 = df['Adj Close'].rolling(window=20).mean() # 수정 종가 20일 이동평균\n",
    "    ma60 = df['Adj Close'].rolling(window=60).mean() # 수정 종가 60일 이동평균\n",
    "\n",
    "\n",
    "    # 이동평균 값 추가하기\n",
    "    df.insert(len(df.columns), \"MA5\", ma5) # 'MA5'라는 열 이름으로 ma5 값 추가\n",
    "    df.insert(len(df.columns), \"MA20\", ma20) # 'MA20'라는 열 이름으로 ma20 값 추가\n",
    "    df.insert(len(df.columns), \"MA60\", ma60) # 'MA60'라는 열 이름으로 ma60 값 추가\n",
    "\n",
    "\n",
    "    # 거래량 5일 이동평균 추가\n",
    "    vma5 = df['Volume'].rolling(window=5).mean() # 거래량의 5일 이동평균 구하기\n",
    "    df.insert(len(df.columns), \"VMA5\", vma5) # 'VMA5'라는 열 이름으로 vma5 값 추가\n",
    "\n",
    "\n",
    "    # --- 이격도 추가 --- #\n",
    "    # 수정 종가 데이터를 5일 이동평균 값으로 나눈 비율\n",
    "    disp5 = (df['Adj Close']/df['MA5'])*100\n",
    "\n",
    "    # 이격도 데이터를 'Disp5'라는 열 이름으로 추가\n",
    "    df.insert(len(df.columns), \"Disp5\", disp5) \n",
    "\n",
    "\n",
    "\n",
    "    # 데이터 확인\n",
    "    print('이동평균 및 이격도가 추가된 주가 데이터')\n",
    "    print(df)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 이동평균선 시각화\n",
    "    print(\"\\n이동평균선 그래프\")\n",
    "    plt.plot(df['Adj Close'], label=\"Adj Close\") # 수정 종가\n",
    "    plt.plot(df['MA5'], label=\"MA5\") # 종가 5일 이동평균\n",
    "    plt.plot(df['MA20'], label=\"MA20\") # 종가 20일 이동평균\n",
    "    plt.plot(df['MA60'], label=\"MA60\") # 종가 60일 이동평균\n",
    "\n",
    "    # 시각화 옵션 코드\n",
    "    # (시각화 강의에서 별도로 다루는 내용입니다)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.grid()\n",
    "    plt.savefig(\"plot2.png\")\n",
    "    elice_utils.send_image(\"plot2.png\")\n",
    "    \n",
    "    \n",
    "    plt.cla()\n",
    "    # 거래량 이동평균값 시각화하기\n",
    "    print(\"\\n거래량 이동평균값 그래프\")\n",
    "    plt.plot(df.index, df['VMA5'], label='VMA5') # 거래량 5일 이동평균\n",
    "    plt.plot(df.index, df['Volume'], label='Volume') # 거래량 데이터\n",
    "\n",
    "    # 시각화 옵션 코드\n",
    "    # (시각화 강의에서 별도로 다루는 내용입니다)\n",
    "    plt.legend(loc='best')\n",
    "    plt.xticks(rotation = 45)\n",
    "    plt.grid()\n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")\n",
    "\n",
    "def train():\n",
    "\n",
    "    # --- 주식 데이터 로드, 전처리, 분할, 모델 학습하기(이전 실습에서 진행) --- #\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "\n",
    "    # 주가의 중간값 계산하기\n",
    "    high_prices = df['High'].values\n",
    "    low_prices = df['Low'].values\n",
    "    mid_prices = (high_prices + low_prices) / 2\n",
    "\n",
    "    # 주가 데이터에 중간 값 요소 추가하기\n",
    "    df['Mid'] = mid_prices\n",
    "\n",
    "    # 종가의 5일 이동평균값을 계산하고 주가 데이터에 추가하기\n",
    "    ma5 = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['MA5'] = ma5\n",
    "\n",
    "    df = df.fillna(0) # 비어있는 값을 모두 0으로 바꾸기\n",
    "\n",
    "    # Date 열를 제거합니다.\n",
    "    df = df.drop('Date', axis = 1)\n",
    "\n",
    "    # 데이터 스케일링(MinMaxScaler 적용)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    fitted = min_max_scaler.fit(df)\n",
    "\n",
    "    output = min_max_scaler.transform(df)\n",
    "    output = pd.DataFrame(output, columns=df.columns, index=list(df.index.values))\n",
    "\n",
    "    # 트레인셋/테스트셋 크기 설정\n",
    "    train_size = int(len(output)* 0.6) # 트레인셋은 전체의 60%\n",
    "    test_size = int(len(output)*0.3) + train_size # 테스트셋은 전체의 30%\n",
    "\n",
    "    #train/test 학습 및 라벨 설정\n",
    "    #종가를 예측하기 위해 종가를 label로 설정\n",
    "    train_x = np.array(output[:train_size])\n",
    "    train_y = np.array(output['Close'][:train_size])\n",
    "    test_x =np.array(output[train_size:test_size])\n",
    "    test_y = np.array(output['Close'][train_size:test_size])\n",
    "    validation_x = np.array(output[test_size:])\n",
    "    validation_y = np.array(output['Close'][test_size:])\n",
    "\n",
    "    # Keras 모델을 생성합니다.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Keras 딥러닝 모델 학습을 위한 파라미터(옵션값)을 설정합니다.\n",
    "    learning_rate = 0.01\n",
    "    training_cnt = 1000\n",
    "    batch_size = 100 \n",
    "    input_size = 8 \n",
    "\n",
    "    # 생성된 딥러닝 모델에 학습용 데이터(train_x)를 넣습니다.\n",
    "    model.add(Dense(input_size, activation='tanh', input_shape=(train_x.shape[1],))) \n",
    "    model.add(Dense(input_size * 3,  activation='tanh')) \n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "    # 데이터를 학습을 진행합니다.\n",
    "    model.compile(optimizer='sgd', loss='mse', metrics=['mae', 'mape','acc'])\n",
    "    model.summary()\n",
    "    history = model.fit(train_x, train_y, epochs=training_cnt,   \n",
    "                        batch_size=batch_size, verbose=1)\n",
    "    val_mse, val_mae, val_mape, val_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "    \n",
    "    \n",
    "def predict():\n",
    "\n",
    "    # --- 학습 결과를 그래프로 확인해봅니다 --- #\n",
    "\n",
    "    # 학습된 모델로부터 테스트 데이터를 예측합니다.\n",
    "    pred = model.predict(test_x)\n",
    "\n",
    "    fig = plt.figure(facecolor='white', figsize=(8, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(test_y, label='True') # 실제 주가\n",
    "    ax.plot(pred, label='Prediction') # 우리가 만든 딥러닝 모델이 예측한 주가\n",
    "    ax.legend()\n",
    "\n",
    "    # 현재까지 그려진 그래프를 시각화\n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eab89a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### 주가 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890b9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py\n",
    "import machine as ma\n",
    "\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    지시사항 1번. 딥러닝 모델의 학습과 예측을 수행해보세요.\n",
    "    \"\"\"\n",
    "    ma.train()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c52ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine.py\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from elice_utils import EliceUtils\n",
    "from tensorflow.keras.layers import Dense, LSTM, ReLU\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, ReLU\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.losses import MSE\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "def rev_min_max_func(scaled_val):\n",
    "\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "    df = df.fillna(0)\n",
    "    max_val = max(df['Close'])\n",
    "    min_val = min(df['Close'])\n",
    "    og_val = (scaled_val * (max_val - min_val)) + min_val\n",
    "    return og_val\n",
    "\n",
    "def data_plot():\n",
    "\n",
    "    # --- 주식 데이터 로드, 전처리, 분할, 모델 학습하기(이전 실습에서 진행) --- #\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "\n",
    "    # 데이터프레임 출력(데이터프레임은 (헹 X 열)로 이루어진 표 형태의 특수한 데이터 타입)\n",
    "    print(df)\n",
    "\n",
    "\n",
    "    # --- 주식 데이터 살펴보기 --- #\n",
    "\n",
    "    print('\\n주식 데이터의 형태를 출력')\n",
    "    print(df.shape)\n",
    "\n",
    "    print('\\n주식 데이터의 정보를 출력')\n",
    "    print(df.info)\n",
    "\n",
    "    print('\\n주식 데이터의 상단 5개 행을 출력')\n",
    "    print(df.head())\n",
    "\n",
    "    print('\\n주식 데이터의 하단 5개 행을 출력')\n",
    "    print(df.tail())\n",
    "\n",
    "    print('\\n주식 데이터의 모든 열을 출력')\n",
    "    print(df.columns)\n",
    "\n",
    "    print('\\n주식 데이터의 요약 통계 자료 출력')\n",
    "    print(df.describe())\n",
    "\n",
    "def train():\n",
    "\n",
    "    # Keras 딥러닝 모델 학습을 위한 파라미터(옵션값)을 설정합니다.\n",
    "    learning_rate = 0.01\n",
    "    epochs = 1000\n",
    "    batch_size = 200\n",
    "    input_size = 5\n",
    "    time_step = 1 \n",
    "\n",
    "    # --- 주식 데이터 로드, 전처리, 분할, 모델 학습하기(이전 실습에서 진행) --- #\n",
    "    df = pd.read_csv('data/stock.csv') \n",
    "\n",
    "    # 주가의 중간값 계산하기\n",
    "    high_prices = df['High'].values\n",
    "    low_prices = df['Low'].values\n",
    "    mid_prices = (high_prices + low_prices) / 2\n",
    "\n",
    "    # 주가 데이터에 중간 값 요소 추가하기\n",
    "    df['Mid'] = mid_prices\n",
    "\n",
    "    # 종가의 5일 이동평균값을 계산하고 주가 데이터에 추가하기\n",
    "    ma5 = df['Adj Close'].rolling(window=5).mean()\n",
    "    df['MA5'] = ma5\n",
    "\n",
    "    df = df.fillna(0) # 비어있는 값을 모두 0으로 바꾸기\n",
    "\n",
    "    # Date 열를 제거합니다.\n",
    "    df = df.drop('Date', axis = 1)\n",
    "\n",
    "    # 데이터 스케일링(MinMaxScaler 적용)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    fitted = min_max_scaler.fit(df)\n",
    "\n",
    "    output = min_max_scaler.transform(df)\n",
    "    output = pd.DataFrame(output, columns=df.columns, index=list(df.index.values))\n",
    "\n",
    "    x_data = output[['Open', 'Low', 'High', 'Mid', 'MA5']]\n",
    "    y_data = output['Close']\n",
    "\n",
    "    train_size = int(len(x_data) * 0.7)\n",
    "    test_size = int(len(y_data) * 0.3) + train_size\n",
    "\n",
    "    train_x = np.array(x_data[:train_size])\n",
    "    train_x = train_x.reshape((train_x.shape[0], time_step, input_size))\n",
    "    train_y = np.array(y_data[:train_size])\n",
    "\n",
    "    \"\"\"\n",
    "    validation_x = np.array(x_data[train_size:validation_size])\n",
    "    validation_x = validation_x.reshape((validation_x.shape[0], time_step, input_size))\n",
    "    validation_y = np.array(y_data[train_size:validation_size])\n",
    "    \"\"\"\n",
    "    test_x = np.array(x_data[train_size:])\n",
    "    test_x = test_x.reshape((test_x.shape[0], time_step, input_size))\n",
    "    test_y = np.array(y_data[train_size:])\n",
    "\n",
    "    # Keras 모델을 생성합니다.\n",
    "    model = Sequential()\n",
    "\n",
    "\n",
    "    # 생성된 딥러닝 모델에 학습용 데이터(train_x)를 넣습니다.\n",
    "    model.add(LSTM(512, input_shape=(time_step, input_size)))\n",
    "    model.add(Dense(1))\n",
    "    model.add(ReLU())\n",
    "    model.compile(optimizer=RMSprop(), loss=MSE, metrics=['mae', 'mape'])\n",
    "\n",
    "    # 데이터를 학습을 진행합니다.\n",
    "    model.summary()\n",
    "    history = model.fit(train_x, train_y, epochs=epochs,   \n",
    "                        batch_size=batch_size, verbose=1)\n",
    "    model.evaluate(test_x, test_y, verbose=0)\n",
    "    \n",
    "\n",
    "    # --- 학습 결과를 그래프로 확인해봅니다 --- #\n",
    "\n",
    "    # 학습된 모델로부터 테스트 데이터를 예측합니다.\n",
    "    pred = model.predict(test_x)\n",
    "\n",
    "    fig = plt.figure(facecolor='white', figsize=(8, 5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(rev_min_max_func(test_y), label='True') # 실제 주가\n",
    "    ax.plot(rev_min_max_func(pred), label='Prediction') # 우리가 만든 딥러닝 모델이 예측한 주가\n",
    "    ax.legend()\n",
    "\n",
    "    # 현재까지 그려진 그래프를 시각화\n",
    "    plt.savefig(\"plot.png\")\n",
    "    elice_utils.send_image(\"plot.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
